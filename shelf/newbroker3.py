#!/usr/bin/env python
# -*-coding: utf8-*-
# newbroker.py
'''
This is the high-performance multiprocessing portion of the orchestration
layer of the PreservationSimulation project.  The broker main module
creates a series of instructions in a queue fed to this module.  This 
module processes all the instructions in the queue as quickly as possible
using all the cores available on the system.  
'''

'''
One root process (like broker) starts two threads.
- 1: start all jobs ("StartJobs") from a list of instructions until 
    none remain, then sleep a short time.
- 2: finish all jobs ("FinishJobs") running until none remain, 
    then sleep a short time.  
Each case is a separate multiprocessing task (like listactor) that receives 
    a list of commands to execute.  
  A single listactor-like case itself a multiprocessing job 
    (mp jobs 3 thru nParallel).
    The case runs a sequence of subprocess jobs, one per command, 
      collects output.
      Case writes logfile where specified, returns list of output lines.
- For each command in the list, the subprocess job executes 
    that command via the shell and receives its stdout as a string, 
    which is splits into a list of lines.
- The multiprocessing job (listactor) appends the outputs of the several 
    commands, each output a list of lines, into a list, and queues 
    that list back to the single parent (broker). 
'''

import multiprocessing
import subprocess 
import threading
import time
import sys
import collections
import re
import itertools
import datetime
import os
import queue
from NewTraceFac import NTRC, ntrace, ntracef


# Named tuples:

# Job process ID and output queue.
tJob = collections.namedtuple("tJob", "procid")
# Line returned from a single command
tLineOut = collections.namedtuple("tLineOut", 
            "callstatus cmdstatus casenr linenr ltext ")
# List of lines returned from list of commands
tLinesOut = collections.namedtuple("tLinesOut", "procname, listoflists")
# Sadistics on how many times we waited for something
tWaitStats = collections.namedtuple("tWaitStats", "ncases slot done inst")


# ==================== subprocess user: do one line ====================

# f n D o O n e L i n e 
@ntracef("DO1L")
def fntDoOneLine(mysLine, mynProc, mynLine):
    """Execute one single-line command.  
    
    Input: single line of command.  
    Output: tuple of the (Popen PIPE return code, command return code, list
     of output lines as strings.
    Contributes line(s) to be in log file.
     Input lines and the first line of output blocks have timestamps;
     other lines in output blocks are indented with spaces.  
    """
    sTimeBegin = fnsGetTimestamp()
    proc = (subprocess.Popen(mysLine
        , stdout=subprocess.PIPE
        , close_fds=True            # The default anyway, I think.  
        , stderr=subprocess.DEVNULL
        , universal_newlines=True
        , shell=True)
        )
    (sProcOut, sProcErr) = proc.communicate()
    proc.stdout.close()
    if not sProcErr: sProcErr = ""
    sTimeEnd = fnsGetTimestamp()
    
    # Format lines for output by timestamping or indenting each line.  
    sOut = ("-"*len(sTimeBegin) + "\n"
            + sTimeBegin + "  " + "$ " + mysLine + "\n")
    lTmpOut1 = sProcOut.rstrip().split("\n")
    lTmpOut2 = [fnsStampLine(sTimeEnd, sLine, (i==0))
                    for i,sLine in enumerate(lTmpOut1)]
    sOut += "\n".join(lTmpOut2)
    sOut += sProcErr.rstrip()
    
    # Collect and return everything to caller.  
    nCmdStat = "n/a - RBL"
    nReturnCode = proc.returncode
    lOut = sOut.split("\n")
    NTRC.ntracef(4, "DO1L", "proc DoOneLine case|%s| line|%s| "
                "sline|%s| lResult|%s|" 
                % (mynProc, mynLine, mysLine, lOut))
    
    return(tLineOut(callstatus=nReturnCode, cmdstatus=nCmdStat
            , linenr=mynLine, casenr=mynProc, ltext=lOut))


# ==================== multiprocessing: DoOneCase ====================

# f n t D o O n e C a s e 
@ntracef("DO1")
def fntDoOneCase(mytInstruction, qToUse):
    """Input: list of instructions generated by the broker for this case; 
     multiprocessing queue through which to report results.
    
    Remove blanks, comments, etc., from the instructions.  Each line that
     is not blank or comment is a command to be executed.  Blanks and 
     comments are written directly into the output.

    Output: list of commands and their output, sent to the supplied queue.
     The text will also be written to a log file for the case.  
    
    This function will be a multiprocessing external process.
    """
    sWhoami = multiprocessing.current_process().name
    NTRC.ntracef(3, "DO1", "proc procname|%s|" % (sWhoami))
    nProc = fnsGetProcessNumber(sWhoami)
    lResults = []                   # list of strings

    # Unpack instruction command list and other items.
    lInstruction = mytInstruction.cmdlist
    (sLogfileDir, sLogfileName) = (mytInstruction.logdir
                                , mytInstruction.logname)

    # Process all command lines of the instruction list and collect results.  
    for nLine, sLine in enumerate(lInstruction):
        if fnbDoNotIgnoreLine(sLine):
            # Genuine line; execute and collect answer line(s).  
            tAnswer = fntDoOneLine(sLine, nProc, nLine)
            (nRtn, nErr, lResult) = (tAnswer.callstatus
                                    , tAnswer.cmdstatus
                                    , tAnswer.ltext)
            lResults.extend(lResult)
            NTRC.ntracef(4, "DO1", "proc DoOneCase case|%s| line|%s| "
                        "lResult|%s|" 
                        % (nProc, nLine, lResult))
        else:
            # Comment or blank line; just append to results.
            lResults.extend([("-"*len(fnsGetTimestamp()))
                            , (fnsGetTimestamp() + "  " + sLine)])
            NTRC.ntracef(4, "DO1", "proc DoOneCase case|%s| line|%s| "
                        "comment|%s|" 
                        % (nProc, nLine, sLine))
    fnWriteLogFile(nProc, (lResults), sLogfileDir, sLogfileName)

    lPrefix = [("BEGIN results from " + sWhoami)]
    lSuffix = [("ENDOF results from " + sWhoami)]
    lResultsToSee = ['\n'] + lPrefix + lResults + lSuffix + ['\n']
    tAnswers = tLinesOut(procname=sWhoami, listoflists=lResultsToSee)
    qToUse.put(tAnswers)
    qToUse.close()
    return (tAnswers)


# f n W r i t e L o g F i l e 
@ntracef("DO1")
def fnWriteLogFile(mynProc, mylContents, mysFileDir, mysFileName):
    sFullName = mysFileDir + "/" + mysFileName
    sContents = '\n'.join(mylContents)
    with (open(sFullName, "w")) as fhOut:
        print(sContents, file=fhOut)

# Debug version of same: write only every tenth log file.
if not (os.getenv("DEBUG", "") == ""):
    @ntracef("DO1")
    def fnWriteLogFile(mynProc, mylContents, mysFileName):
        sContents = '\n'.join(mylContents)
        if fnIntPlease(mynProc) % 10 == 0:
            with (open(mysFileName, "w")) as fhOut:
                print(sContents, file=fhOut)


# ==================== vanilla function: RunEverything ====================

# f n t R u n E v e r y t h i n g 
def fntRunEverything(mygl, qInstr, fnbQEnd, nWaitMsec, nWaitHowMany):
    '''Start an async job for each case.  Limit number of concurrent jobs
    to the size of the ltJobs vector.  
    When a job completes, ship its output upline and remove it from 
    the active lists.  
    
    Two separate threads:
    - Wait for an empty slot; get an instruction, start an async job.
    - Wait for an active job to complete and remove it from lists.  
    '''
    # Fill the list of jobs with empties.
    for i in range(mygl.nParallel + 1): mygl.ltJobs.append(None)
    mygl.lockJobList = threading.Lock()
    mygl.lockPrint = threading.Lock()

    # Create and start new threads
    NTRC.ntracef(5, "RUN", "proc make thread instances")
    mygl.thrStart = CStartAllCases(mygl, nWaitMsec, nWaitHowMany
                            , qInstr, fnbQEnd)
    mygl.thrEnd = CEndAllCases(mygl, nWaitMsec, )
    mygl.llsFullOutput = [["",""]]
    #mygl.thrStart.start()
    #mygl.thrEnd.start()
    
    # Wait until all jobs have started and finished.
    # Make sure the thread is running, then wait for it to finish.
    while not mygl.thrStart.bStarted():
        time.sleep(nWaitMsec)
    if (mygl.thrStart.is_alive()):
        mygl.thrStart.join()     # Runs out of instructions.

    while not mygl.thrEnd.bStarted():
        time.sleep(nWaitMsec)
    if (mygl.thrEnd.is_alive()):
        mygl.thrEnd.join()     # Runs out of instructions.
    
    return tWaitStats(ncases=mygl.nCasesDone
                , slot=mygl.nWaitedForSlot
                , done=mygl.nWaitedForDone
                , inst=mygl.nWaitedForInstr)


# =================== vanilla function: fnvStartThreads ====================

# f n v S t a r t T h r e a d s 
def fnvStartThreads(mygl):
    if not mygl.thrStart.is_alive(): mygl.thrStart.start()
    if not mygl.thrEnd.is_alive(): mygl.thrEnd.start()


# ==================== thread: DoAllCases ====================

# c l a s s   C S t a r t A l l C a s e s 
class CStartAllCases(threading.Thread):
    """pseudocode:
    arg gives list of instructions,
     each instruction is list of command lines
    foreach instruction in list *shared data, locked*
      wait for opening
      create queue for result
      create process to do instruction
      append (process,queue) to job list *shared data, locked*
    set end flag, return
    """

    #@ntracef("STRT")
    def __init__(self, mygl 
                , mynWaitMsec, mynWaitHowMany
                , myqInstructions, myfnbEnd
                ):
        threading.Thread.__init__(self, name="startall")
        self.gl = mygl
        self.nWaitMsec = mynWaitMsec
        self.nWaitHowMany = mynWaitHowMany
        self.nCounter = itertools.count(1)
        self.nProcess = 0
        self.qInstructions = myqInstructions
        self.fnbEnd = myfnbEnd
        self.bStarted = False
        NTRC.ntracef(2, "STRT", "exit init gl|%s| instrs|%s|" 
                % (self.gl, self.qInstructions))


    # r u n   function for the StartAllCases thread
    @ntracef("STRT")
    def run(self):
        self.bStarted = True
        while True:
            # Empty the queue of pending instructions.  
            while not self.qInstructions.empty():
                tOneInstr = self.qInstructions.get()
                with self.gl.lockPrint:
                    NTRC.ntracef(3, "STRT", "proc instr|%s|" % (repr(tOneInstr)))
                lLines, sLogFilename = tOneInstr.cmdlist, tOneInstr.logname
                dInstr, sRunId = tOneInstr.casedict, tOneInstr.runid
                # Wait until there is an empty slot.  
                bStatus = (fnbWaitForOpening(self.gl, self.nWaitMsec
                            , self.nWaitHowMany)
                            )

                # L O C K   J O B S
                with self.gl.lockJobList:
                    # Find an empty slot in the jobs list.
                    lEmptySlots = [idx for (idx,x) in enumerate(self.gl.ltJobs) 
                                    if not x]
                    assert len(lEmptySlots) > 0, ("Supposed to be an empty slot"
                                "for new case, but I can\'t find one.")
                    idxEmpty = lEmptySlots[0]

                    # Create resources for the job.        
                    qOut = multiprocessing.Queue()
                    nJob = next(self.nCounter)
                    with self.gl.lockPrint:
                        fnvReportCaseInstructions(tOneInstr)
                        NTRC.ntracef(0, "STRT", "proc case|%s| start |%s|" 
                                    % (nJob, sRunId))
                    proc = multiprocessing.Process(target=fntDoOneCase
                                    , args=(tOneInstr, qOut)
                                    )
                    tThisJob = tJob(procid=nJob, )

                    # Save job in empty slot of list, and save dict
                    #  entries to get proc and queue.
                    self.gl.dId2Proc[nJob] = proc
                    self.gl.dId2Queue[nJob] = qOut
                    # Save job info in jobs list.
                    self.gl.ltJobs[idxEmpty] = tThisJob
                    with self.gl.lockPrint:
                        NTRC.ntracef(3, "STRT", "proc startall go slot|%s| njob|%s|" 
                                    % (idxEmpty, nJob))
                    proc.start()
                    self.nProcess += 1
                    self.gl.nCasesStarted += 1
                # E N D L O C K   J O B S 
            # ENDWHILE QUEUE NOT EMPTY
            
            # Queue is empty.  Why?
            if self.fnbEnd():
                
                # End of instruction stream.  Finish up.  
                self.gl.bThatsAllFolks = True
                self.gl.nCasesTotal = self.gl.nCasesStarted
                with self.gl.lockPrint:
                    NTRC.ntracef(1, "STRT", "proc startall "
                            "exhausted instructions nprocess|%s|" 
                            % (self.nProcess))
                break   # End the thread!
            else:

                # Temporarily empty.  Wait a while and retry.  
                self.gl.nWaitedForInstr += 1
                NTRC.ntracef(3, "STRT", "proc waiting for instr "
                            "nwait|%s| " 
                            % (self.gl.nWaitedForInstr))
                time.sleep(self.nWaitMsec)
                #continue    # Go back and try it again.
            NTRC.ntracef(3, "STRT", "proc end of while queue nonempty")
            #continue        #???
        NTRC.ntracef(3, "STRT", "proc end of while forever")
        # ENDWHILE TRUE


    # m b S t a r t e d 
    def mbStarted(self):
        return self.bStarted

    
# ==================== thread: EndAllCases ====================

# c l a s s   C E n d A l l C a s e s 
class CEndAllCases(threading.Thread):
    """pseudocode:
    while forever
        get list of (job,queue) *shared data, locked*
        while noempty, foreach in list of job.notalive
            join job to make sure it's dead
            empty its queue
            append output to big string
            pop job from list *shared data, locked*
        if empty
            if instruction list empty
                return big string
                break
            else
                wait a few milliseconds
                continue
    """

    #@ntracef("END")
    def __init__(self, mygl, mynWaitMsec):
        threading.Thread.__init__(self, name="endall")
        self.gl = mygl
        self.nWaitMsec = mynWaitMsec
        self.llsFullOutput = list()
        self.bStarted = False
        NTRC.ntracef(2, "END", "exit init gl|%s| wait|%s|" 
                    % (self.gl, self.nWaitMsec))


# r u n   function for EndAllCases thread
    @ntracef("END")
    def run(self):
        self.bStarted = True
        NTRC.ntracef(5, "END", "proc run ltJobs|%s|" % (self.gl.ltJobs))
        nCasesDone = 0
        self.gl.nWaitedForDone = 0
        while True:
            # L O C K 
            with self.gl.lockJobList:
                with self.gl.lockPrint:
                    NTRC.ntracef(3, "END", "proc ltJobs|%s|" % (self.gl.ltJobs))
                    ltActiveJobs = [(idx,tJob) for idx,tJob in 
                                    enumerate(self.gl.ltJobs) if tJob]
                    NTRC.ntracef(3, "END", "proc ltActiveJobs|%s|" 
                                % (ltActiveJobs))
                for idxtJob in ltActiveJobs:
                    idx,tJob = idxtJob
                    nJob = tJob.procid
                    proc = self.gl.dId2Proc[nJob]
                    if not proc.is_alive():
                        with self.gl.lockPrint:
                            NTRC.ntracef(3, "END", "proc endall found done "
                                        "ltJobs[%s]=procid|%s|=|%s| alive?|%s|" 
                                        % (idx, nJob, proc, proc.is_alive()))
                            # Job listed as still baking but reports that it is done.
                        # Wait until it is fully baked.
                        proc.join()
                        with self.gl.lockPrint:
                            NTRC.ntracef(0, "END", "proc case|%s| end   " 
                                        % (nJob))
                        # Get its output for the full debug list.
                        queue = self.gl.dId2Queue[nJob]
                        lQOutput = []
                        while not queue.empty():
                            lLinesOut = queue.get().listoflists
                            lQOutput.append(lLinesOut)
                        queue.close()
                        if self.gl.bDebugPrint:
                            with self.gl.lockPrint:
                                NTRC.ntracef(5, "END", "proc lQOutput from q|%s|" 
                                                % (lQOutput))
                                self.llsFullOutput.extend(lQOutput)
                                NTRC.ntracef(5, "END", "proc lOutput from q|%s|" 
                                            % (self.llsFullOutput))
                            # Remove job from active list and Id-dicts.
                        # If the queue objects are still in the dId2Queue dict,
                        #  the pipe remains open, oops.  
                        self.gl.ltJobs[idx] = None
                        self.gl.dId2Proc.pop(nJob)
                        self.gl.dId2Queue.pop(nJob)
                        nCasesDone += 1
                        self.gl.nCasesDone += 1
                        with self.gl.lockPrint:
                            NTRC.ntracef(3, "END", "proc job completed ndone|%s|" 
                                        % (self.gl.nCasesDone))
                    else:
                        with self.gl.lockPrint:
                            NTRC.ntracef(3, "END", "proc job alive "
                                        "ltJobs[%s]=procid|%s|=|%s|"
                                        % (idx, nJob, proc))

                with self.gl.lockPrint:
                    NTRC.ntracef(3, "END", "proc end for-activejobs1"
                                " thatsall?|%s| ndone|%s| nstarted|%s|" 
                                % (self.gl.bThatsAllFolks
                                , self.gl.nCasesDone, self.gl.nCasesStarted))

            # Now unlock and check for end of loop.
            if (self.gl.bThatsAllFolks 
                and self.gl.nCasesDone == self.gl.nCasesTotal):
                with self.gl.lockPrint:
                    NTRC.ntracef(0, "END", "proc end of all jobs, "
                                "ndone|%s| nwaits instr|%s| slot|%s| done|%s|" 
                                % (nCasesDone
                                , self.gl.nWaitedForInstr
                                , self.gl.nWaitedForSlot
                                , self.gl.nWaitedForDone
                                ))
                break
            else:
                self.gl.nWaitedForDone += 1
                with self.gl.lockPrint:
                    NTRC.ntracef(3, "END", "proc end for-activejobs2 wait, "
                                "ndone|%s| nwaits|%s|" 
                                % (nCasesDone, self.gl.nWaitedForDone))
                time.sleep(self.nWaitMsec / 1000.0)
                continue
            # E N D L O C K 

        # llsFullOutput is a list of list of strings, where
        #  the inner list is lines output from commands for
        #  one job, more or less, with prefix and suffix 
        #  and comments, too.
        # Paste the whole thing together into a yuge list of lines.
        if self.gl.bDebugPrint:
            sFullOutput = ""
            for lJobOut in self.llsFullOutput:
                sJobOut = "\n".join(lJobOut)
                sFullOutput += sJobOut
            NTRC.ntracef(5, "END", "proc sFullOutput|%s|" % (sFullOutput))


    # m b S t a r t e d 
    def mbStarted(self):
        return self.bStarted


# ==================== utilities ====================

# f n v R e p o r t C a s e I n s t r u c t i o n s 
def fnvReportCaseInstructions(mytInstr):
    '''Print the details for the case: 
    '''
    (runid, dInstr) = (mytInstr.runid, mytInstr.casedict)
    (lCmds) = (mytInstr.cmdlist)
    (sLogDir, sLogName) = (mytInstr.logdir, mytInstr.logname)
    
    (nCopies, nLifem) = (dInstr["nCopies"],dInstr["nLifem"])
    (nAuditFreq, nAuditSegments) = (dInstr["nAuditFreq"]
                                    , dInstr["nAuditSegments"])
    (nShockFreq, nShockImpact, nShockSpan) = (dInstr["nShockFreq"]
                                    , dInstr["nShockImpact"]
                                    , dInstr["nShockSpan"]
                                    )
    
    NTRC.ntracef(0, "STRT", "proc main commands run|%s| "
        "ncopies|%s| lifem|%s| audit|%s|seg|%s|"
        "\n1-|%s|\n2-dir|%s| log|%s|" 
        % (runid, nCopies, nLifem, nAuditFreq, nAuditSegments, 
        lCmds, sLogDir, sLogName)
        )
    return


# f n b D o N o t I g n o r e L i n e 
def fnbDoNotIgnoreLine(mysLine):
    '''
    True if not a comment or blank line.
    '''
    # Ignore comment and blank lines, but take all others.
    return (not re.match("^\s*#", mysLine)) and (not re.match("^\s*$", mysLine))


# f n I n t P l e a s e 
def fnIntPlease(mysIn):
    try:
        val = int(mysIn)
    except ValueError:
        val = mysIn
    return val


# f n n H o w M a n y A l i v e 
@ntracef("MANY", level=5)
def fnnHowManyAlive(gl):
    '''How many empty slots in the jobs list?
    The criterion is just empty (=None) vs anything else.
    '''
    nAlive = len([1 for tJob in gl.ltJobs if tJob])
    return nAlive


# f n b W a i t F o r O p e n i n g 
@ntracef("WAIT")
def fnbWaitForOpening(gl, mynWaitTimeMsec, mynWaitMax):
    '''How many active jobs?  If maxed out, wait for an empty slot 
    and try again.
    '''
    nWait = mynWaitMax
    while nWait:
        nAlive = fnnHowManyAlive(gl)
        if nAlive < gl.nParallel:
            break
        else:
            nWait -= 1
            gl.nWaitedForSlot += 1
            if gl.bDebugPrint:
                print(".", end='')          # DEBUG
            time.sleep(mynWaitTimeMsec / 1000.0)
            NTRC.ntracef(5, "WAIT", "proc waitforopening timesleft|%s| "
                        "nwaited|%s|" 
                        % (nWait, gl.nWaitedForSlot))

    # Have we waited too long for an opening?
    if nWait <= 0:
        raise ValueError("Waited too long for empty job slot.")
    else:
        return (nWait > 0)


# f n s G e t P r o c e s s N u m b e r 
def fnsGetProcessNumber(mysProcName):
    '''Extract the process number, which is the same as the case number
    in this case, so we can use its abbreviated form in logs and traces.
    '''
    sProcNum = re.match("Process-(\d+)", mysProcName).group(1)
    return sProcNum if sProcNum else ""


# f n s G e t T i m e s t a m p 
def fnsGetTimestamp():
    '''Return timestamp with milliseconds.
    '''
    return datetime.datetime.now().strftime('%Y%m%d_%H%M%S.%f')[:-3]
#    return datetime.datetime.now().strftime('%Y%m%d_%H%M%S')   # without msec


# f n s S t a m p L i n e 
def fnsStampLine(mysStamp, mysLine, mybFirstLine):
    """To indent paras of lines where the timestamp appears only
    on the first line, blanks to indent all the others.  
    """
    if mybFirstLine:
        return fnsGetTimestamp() + "  " + mysLine
    else:
        return " "*len(fnsGetTimestamp()) + "  " + mysLine


# ==================== in main process ====================

# E n t r y   p o i n t 
if __name__ == "__main__":

    # ==================== m a i n N e w B r o k e r ====================
    @ntrace
    def mainNewBroker(gl):

        NTRC.ntrace(3, "proc params ncases|%s| nparallel|%s| "
                        "nwaitmsec|%s| nwaitmany|%s|" 
                    % (gl.nCases, gl.nParallel, gl.nWaitMsec, gl.nWaitHowMany))
        
        # Main loop
        
        # Create list of instructions.  Each instruction is a list of 
        #  command strings.
        lLinesTemp = [sLine.lstrip() 
                    for sLine in sTempListOfCommands.split('\n')
                    ]
        # And make a list of instructions for each case.
        llsInstructionsTemp =  [lLinesTemp] * gl.nCases

        tTmp = fntRunEverything(gl, iter(llsInstructionsTemp)
                            , gl.nWaitMsec, gl.nWaitHowMany)
        (gl.nWaitedForSlot, gl.nWaitedForDone) = (tTmp.slot, tTmp.done)
        return []   # used to be llOut in early proto


    # m a i n 
    def main(gl):
        NTRC.ntrace(0, "Starting...")
        tStart = datetime.datetime.now()
        llFullOutput = mainNewBroker(gl)

        if gl.bDebugPrint:
            # Print all the crap that comes back.  
            print("---------begin cases----------")
            for lCase in llFullOutput:
                sCaseOut = ""
                NTRC.ntrace(3, "proc fromq lCase|%s|" % (lCase))
                sCaseOut = '\n'.join(lCase)
                print(sCaseOut)
                print("--------------")
            print("---------end cases----------")
        NTRC.ntrace(0, "Finished nWaitedForSlot|%s| nWaitedForDone|%s|" 
                    % (gl.nWaitedForSlot, gl.nWaitedForDone))
        tEnd = datetime.datetime.now()
        tDif = tEnd - tStart
        tDifMuSec = float((tDif.seconds * 1E6) + tDif.microseconds)
        NTRC.ntrace(0, "Time total|%.3f|sec cases|%s| parallel|%s| "
                    "per case|%.0f|msec" 
                    % (tDifMuSec/1E6, gl.nCases, gl.nParallel,
                    tDifMuSec/gl.nCases/1E3))


    gl = CGlobal()      # Instantiate global data region.  

    # Get CLI args or their default values.
    '''
    argv[1] = total number of cases to run; default 20.
    argv[2] = max number to run simultaneously; default 8.
    argv[3] = sleep interval in loop waiting for open process slot; 
                default 50 (msec).
    argv[4] = max number of times to sleep waiting for an open process slot; 
                default = 100.
    argv[5] = if present, causes debug printing of all the job output.
    '''
    gl.nCases = 20 
    gl.nParallel = 8
    gl.nWaitMsec = 50
    gl.nWaitHowMany = 10000
    nArgs = len(sys.argv)
    if nArgs > 1: gl.nCases = int(sys.argv[1]) 
    if nArgs > 2: gl.nParallel = int(sys.argv[2]) 
    if nArgs > 3: gl.nWaitMsec = int(sys.argv[3]) 
    if nArgs > 4: gl.nWaitHowMany = int(sys.argv[4]) 
    if nArgs > 5: gl.bDebugPrint = True

    """ Temp hack to make instructions for debugging.
    """
    try:
        with open("instructions.txt", "r") as fhIn:
            sTempListOfCommands = fhIn.read()
    except FileNotFoundError:
        # Cheap version.
        sTempListOfCommands = '''
            date +%Y%m%d_%H%M%S.%3N
            # this is comment 1
            ls | head -3
            pwd
            # this is comment 2
            
            # and a blank line before and after this one
            
            python3 -V
            python3 fib.py 35       # spend some cpu time
            cat /proc/version
            cat /proc/cpuinfo | grep processor |wc -l   
            ps | grep python | grep -v grep
            date +%Y%m%d_%H%M%S.%3N
        '''
        # Really cheap version.  
        sTempListOfCommands = '''
            date +%Y%m%d_%H%M%S.%3N
        '''

    sys.exit(main(gl))


# Edit history:
# 20181105  RBL First version that actually works, yay.  Finally remembered
#                to remove references to the queue pipes from the ID-dict
#                so that now the pipe files get closed.  
# 20181106  RBL Add nWaitedForDone to the end stats.
#               Add timestamps to log file for all lines including comments.
# 20181110  RBL Fold RunAll into the root process; it would be too hard to 
#                pass the instruction generator through pipes.  
# 20181111  RBL Clean up references to gl, which caused some things not
#                to be accounted properly.  
# 20181113  RBL Integrate as module imported into broker2.
#               Need to remove the standalone and obsolete global code someday.
# 20181117  RBL Add tracing in END loop; hangs on AWS.
#               Add print-locks around all traces in STRT and END.
# 20181118  RBL Restructure the startall-run to use a plain queue instead
#                of expecting a huge list of instructions.  
# 20190317  RBL Fix the thread-start logic for Start and End.  
# 
# 

#END
