# broker2commandlist.txt
#
# Sequence of instructions to be expanded and executed by the broker for each
#  instruction chosen from the database.
# Values will be substituted into these commands from the instructions 
#  (so the field names better match precisely)
#  and then the sequence will be packaged into a .cmds file 
#  and a process spawned to execute the commands when a core is free.  

# These commands must be SINGLE LINES.  No backslash-return foolishness.
# As usual, blank lines and comment lines will be removed from the sequence.

#---------------------------------------------

# For the record, get start time.
date +%Y%m%d_%H%M%S.%3N

# Make sure we have the right version of bash and Python.
bash --version
python3 -V
python2 -c 'import sys; print(sys.version); print(sys.version_info)'
python3 -c 'import sys; print(sys.version); print(sys.version_info)'

# Actual simulation command.
# Be sure when selecting the instruction set for a long run that you
#  include selection values or ranges for ALL the parameters 
#  to eliminate any irrelevant instructions.  
#  For instance, stating  --glitchfreq=0  is NOT enough to force no 
#  glitch instructions to be expanded.  You must also include
#  --glitchimpact=100 --glitchdecay=0 --glitchmaxlife=0 
#  Otherwise redundant instructions will be fetched and time will be wasted. 
#  Similarly for shock params: shockfreq=0 does not suffice to filter out
#  all possibly redundant instructions.  
# Singletons such as docsize and audittype can be ignored at one's risk.  
#  But will they always remain singletons?  No, they didn't.  
#  And similarly for docsize, simlen, ndocs, etc.  Specify *every*thing.  

# Turn off any tracing that might have been going on in the broker.
#  This could poison any python programs by exploding their log files.  
(export TRACE_LEVEL=0; export TRACE_PRODUCTION=YES; unset TRACE_FACIL; unset TRACE_TARGET; unset DEBUG; python2 main.py {sFamilyDir} {sSpecificDir} {nSimLength} {nRandomseed} --ncopies={nCopies} --lifek={nLifem}000 --audit={nAuditFreq} --auditsegments={nAuditSegments} --audittype={sAuditType} --glitchfreq={nGlitchFreq} --glitchimpact={nGlitchImpact} --glitchdecay={nGlitchDecay} --glitchmaxlife={nGlitchMaxlife} --glitchspan={nGlitchSpan} --serverdefaultlife={nServerDefaultLife} --shockfreq={nShockFreq} --shockimpact={nShockImpact} --shockmaxlife={nShockMaxlife} --shockspan={nShockSpan} --shelfsize={nShelfSize} --ndocuments={nDocuments} --smalldoc={nDocSize} --pctsmall=100 {sShortLogOption} --mongoid=\'{_id}\'  >  {sFamilyDir}/{sSpecificDir}/log/{sShelfLogFileName}.log  2>&1)

# Slow things down here to avoid maybe looking at the log file
#  before it has finished being written to disk.  
#  With many processes executing in parallel, sometimes the 
#  execution gets ahead of emptying the buffers.
# Wait until the log file is actually done.
# 
# 20181112  RBL Change for python3 and newbroker:
#                Remove all the slow-down timing loops.  Eventually, 
#                remove all such commands and the commentary surrounding them.  
#               Specify python2 for the main simulation, even though we
#                now have an experimental version that runs on p3.
#                newbroker and its trace run on p3.
# 

date +%Y%m%d_%H%M%S.%3N

# Wait for end of log file to be written; else risk "nolinefound" errors 
#  in extraction.  This should not be necessary, but apparently is.  
while true; do if [ -z $(tail -3 {sFamilyDir}/{sSpecificDir}/log/{sShelfLogFileName}.log | grep "ENDENDEND" | wc -l) ]; then sleep 0.100; else sleep 0.01; break; fi; done 
date +%Y%m%d_%H%M%S.%3N

# Data extraction command.  Header must be forced here so that the 
#  cleanup program can label the data fields correctly.

(export TRACE_LEVEL=0; export TRACE_PRODUCTION=YES; unset TRACE_FACIL; unset TRACE_TARGET; python2 extractvalues2.py --header --separator=' ' hl-extractinstructions.txt {sFamilyDir}/{sSpecificDir}/log/{sShelfLogFileName}.log  |  tee  {sFamilyDir}/{sSpecificDir}/ext/{sShelfLogFileName}.ext)

date +%Y%m%d_%H%M%S.%3N

# Data cleanup command.  Appends line from extract file to output file, 
#  and usually deletes the extract file and records it in the done collection.

(export TRACE_LEVEL=0; export TRACE_PRODUCTION=YES; unset TRACE_FACIL; unset TRACE_TARGET; python2 datacleanup.py {sSearchDbProgressCollectionName} {sSearchDbDoneCollectionName} {sFamilyDir}/{sSpecificDir}/ext/{sShelfLogFileName}.ext {sFamilyDir}/{sSpecificDir}/dat/GiantOutput_00.txt  --separator=' ' --donotdelete=N)

# End time
date +%Y%m%d_%H%M%S.%3N


# Edit history:
# 20181111  RBL Copied from original, revised to change python versions as 
#                needed.
# 20181114  RBL Put changes in environment vars in the subprocess with 
#                the python main.py program.  Never really did work
#                the other way, flat forehead.  
# 20181117  RBL Add TRACE_PRODUCTION=YES to the main.py invocation.  
#               Add unset TRACE_others for extract and cleanup.
# 20190121  RBL Add wait for end of log file after main.py run, grumble.  
#                I thought this had been fixed with newbroker, but 
#                apparently Linux is still capable of thinking a job is 
#                completed before its files have been written.  
# 

#END