How to run on AWS
                RBL 20170519
                updated many times, most recently 20181114.


START AWS EC2 MACHINE


Start EC2 machine
- browse to 
    https://mitescience.signin.aws.amazon.com/console 
  or similar, maybe something like rlandau@mitescience dot idontknowwhat.
- account = mitescience
- name = rlandau (NOTE: no b)
- password
- select EC2.
- select instances.
- select the instance  "RBL07A 32core c3.8xl 200GB 20160228 USEME"
    (NOT the copy nor the spare).
- examine State Transition: you want the last instance you closed:
    when launched, when shut down.
- right click, Instance State, Start, yes I'm sure.
- wait for IP addr to show up under IPv4 Public IP, copy to clipboard.
- there is some magic setting that one needs to use in the security section
    that enables an outside browser to access the EC2 system.
    I forget what setting, sorry.  ask stackoverflow.  I did.  


CONNECT


with WinSCP
- select session RBL07A(1)
- protocol SFTP
- Edit
- change IP address; you probably have to use the entire DNS address, 
    something like ec2-107-21-158-226.compute-1.amazonaws.com
- user ubuntu is okay
- Save
- Login
- accept unknown cert from server IP
- enter passphrase
- remember that ctrl-R refreshes only the currently viewed remote dir, and
    does not include its children.  this is a real pain.
- remember that some result directories and files may have been pulled along
    with the source code, so delete any result dirs that you will not use.


with putty
- session RBL07A something
- Load
- change IP address
- Save
- Open
- login as: ubuntu (not as ec2-user, as some suggest)
- enter passphrase

- if you want more than one session, ask for Duplicate Session and login, or
    ask for New Session and repeat these steps.


with ssh
- Cygwin window
- SSH -i aws/<whatever-id-file.pem> ubuntu@<IP-addr>
- yes, continue and accept new fingerprint
- be sure to use correct key file, currently aws/RBL20150827_kp01.pem.


LOAD SOFTWARE


Copy AWS install script
- in WinSCP or FileZilla or whatever
- make sure that the source dir has a recent pull from github on it
- copy file shelf/AWS_shelf_installscript.sh from a development system
- yes, overwrite
- copy the install script onto ~


Erase old software
- The AWS_shelf_installscript.sh has an option to do this for you
        sh AWS_shelf_installscript.sh CLEAROLD
  Note that the option name is all upper case.  Anything else will generate 
  a Usage blurb and exit.
  The script will continue with the installation after clearing the files.
- If you wish to do this manually, here are the commands:
        cd ~                    (the ~ dir should be /home/ubuntu)
        rm -rf working
        rm -rf shelfenv


Run AWS install script to install all the software
- sh AWS_shelf_installscript.sh   or maybe   sh -x Aws_shelf_installscript.sh
- wait a couple minutes
- if the script goes right to the end, saying that it has written the startup
    script, then everything worked.
- if it stops somewhere in the middle, well, then we have some investigation
    to do.


TO RUN A (git) BRANCH OTHER THAN 'MASTER'


The AWS install retrieves only the master branch from github.  To retrieve
a different branch, e.g., for testing, you have to getch that branch
manually from the github remote repository.  

The commands are:

    cd working
    git remote add https://github.com/MIT-Informatics/PreservationSimulation
    git fetch
    git checkout theotherbranchname
    cd ..

And you will then be operating on that new branch of the source code.  
You can then run the startup script to load the web server.  


PREPARING FOR A RUN


Remove any pulled directories of moldy, old results that you don't want.
- below working/ remove any directories that have names you want to use
- rm -rf working/shocks   for instance


Create a directory tree for test results
- cd working/shelf (probably in another SSH or putty session)
- sh setupfamilydir.sh <familydir> <specificdir>
- OR use the setup page of the form, see below.
- (This can be done later with the Setup page of the web form.)

Remove old history from MongoDB
- python dblistdatabases.py   to get names of dbs.  The one used for 
    all UI-launched simulations is   brokeradmin.
- python dblistcollections brokeradmin
- python dbclearcollection brokeradmin done
- python dbdumpdonekeys brokeradmin   just to be sure.


Or, just clear all the done records
- sh dbcleardone.sh
- (This can be done later with the Setup page of the web form.)


Turn on broker logging
- in shelf dir
- sh brokercommandlog_enable.sh
- (This is done automatically by the startup.sh script.)


WEB-BASED OPERATION


Start software
- the console session should still be in ~; go back if necessary with cd
- . startup.sh  (yes,  dot space startup.sh  ) (Ubuntu appears not to have
    the "source" shell command.)
- mongod should already be running
- this creates a new form and starts up the bottle web server with it.


Get web form used to submit input parameters for runs
- browse from your system to <aws-ip-addr>:8080


Setup output directory tree for results.
- select "Go to setup page" option on form.
- fill in family and specific directory names.
- if needed, select option to clear output directory.  You almost always
    want to do this.
- if desired, select option to clear all "done" records from admin db.  You 
    almost always want to do this.  


Run broker for a single range of cases.
- fill in the form and submit.
- be sure to check ALL the fields, esp. server default life, numbers of copies,
   lifetimes, shock and glitch frequencies, impacts, durations.  
- useful trick: set seeds=1 and TEST mode on to count the number of cases, then 
    set seeds=21 or whatever and TEST mode off to do the actual run.  
- Usually, you want to run detached, so check that box and supply a filename
    for the logfile to be produced.  You want to include the date and probably
    the purpose of the runs.  


Pick up the results
- cd familydir/specificdir/dat
- check that the GiantOutput file does not have any nolinefound errors in it.
    the nolinefound errors are usually caused by running out of servers
    due to deaths related to shocks.  the set of servers has already 
    been increased twice, and is now beyond a plausible scenario: are there
    really sixty -- or even thirty or forty -- cloud storage vendors
    to choose from?  if you see these errors, check the log files.
- rename the GiantOutput_00.txt file appropriately to reflect its contents
- zip samenameusedforgiantoutput_logs.zip ../log/*.log
- copy the output and logzip files back to dirs on the home system
- also copy the broker log if you enabled it.
- also copy the detached-run log if you used that.  


Poweroff and/or stop the EC instance when you're done
- sudo poweroff now
- If the instance is left running, even with the OS halted, AWS charges $.  
    Don't do that.  shutdown does not suffice to stop the charges.  


Things to note about the form:
- The Setup page is accessible from the form.  That is the easy way
    to create (and clear) the output directory tree, and to 
    clear the "done" records from the run history database.  
- Some of the fields are marked "Required" and they really are.  
    If you don't do something sensible with those fields, the 
    simulation will not run correctly if it runs at all.  
- Many of the fields in the form are multi-select.  Control-click to 
    select another item in addition to the current selection.  
- Many of the fields have values selected by default.  These may or
    may not be good values for your particular run.  
- If you are simulating shocks, the "Expected half-life of servers" 
    field must be non-zero.  This field is marked as required, but
    that is easy to miss.  
- "Short logs" include only the input parameters and results.  They do
    not include any details during the simulation.  As a result, they 
    are much shorter and faster.  However, if you need to see details
    to understand why the results are what they are, or for (heaven 
    forbid) debugging purposes, then turn off this option.  
- The "TEST only" button will produce a listing of what *would* have 
    occurred if this were a real simulation run.  This is useful to 
    examine the range of parameters for oversights that might cause
    too many or too few cases to be run.  
- The simulation can be run offline so that your browser need not stay
    connected to the AWS server all the time.  This is highly recommended
    for long runs; otherwise a hiccup in communications can cause the 
    sequence of runs to be truncated.  (Because the "done" history is 
    kept up-to-date, simply restarting the run with the same parameters
    will pick up where it left off in the sequence.)  You can specify a
    name for the log file you in which you wish to collect results, or a
    filename will be selected and the results appended to that file (in
    shelf/tmp).  
- If the sequence is running offline with output going to a log file like
    brokerlog_<somepurposeorother>_<date>.log, it will finish even if the IP
    connection is broken.  
- If you need to stop the broker sequence in the middle for some reason, 
    find the process that is running "python broker.py" or similar, and
    kill that process by PID.  You can find the process ID with htop or 
    ps aux or similar command.  


MANUAL OPERATION


If you need to run broker manually for large series of cases:
(Note: The new web UI permits multiple selection for almost all of the 
parameters.  This should make it unlikely that you will ever have 
to issue a broker command through the CLI.  But if you insist....)


Prepare the environment for a long broker run
- cd working/shelf
- BE SURE to activate shelfenv: from ~   . shelfenv/bin/activate  
- BE SURE to specify   export TRACE_PRODUCTION=YES   for efficiency.
- ENSURE that you have enough disk space.  WARNING: long log files for a 
    few thousand runs will consume 50-100GB of disk space, and you don't
    want to run out in the middle of a long sequence of runs.  


Prepare a broker command
- python broker.py -h and read the help on all the options
- best way to do this is use the form, look at the command issued by the form 
    on a test-only case (--listonly), copy the CLI command to an editor, 
    and change the params you want there.  
- trying to type the entire command with twenty or thirty options is doomed
    to disappointment.
- check the form's broker output listing to see that the number of cores is 
    maxed out for your configuration.
- typical editing changes to add cases (use JSON syntax): examples:
    range: --ncopies='{"$gte" : 1 , "$lte" : 5}'
    list: --shockfreq='[5000, 10000, 20000]'
    spacing optional, i think
- BE SURE to specify short logs (--shortlog) for most production runs; 
    long log files consume 2x the cputime and walltime and tons of disk space.
- REMEMBER to remove the --listonly when you finally want to run.
- BE SURE to aim the broker output entirely to a log file, so that your 
    session does not need to remain connected the entire time.
- use --nseeds=1 to assess how many cases there are with a short listing.
- BE SURE to correct the number of seeds to 21 or 101 or whatever
    before running production.
- BE SURE to remember the ampersand '&' after the broker command so that 
    it runs in a subshell, leaving your session free to tail the dat file.
- e.g., 
    python broker.py inprogress done --familydir=../hl \
        --specificdir=glitchvslifeAWS --ncopies='{"$gte":1, "$lte":5}' \
        --lifem='{"$gte":10, "$lte":1000}' --serverdefaultlife='[0,100000]' \
        --auditfreq=10000 --auditsegments=1 --audittype=TOTAL \
        --glitchfreq='[0,3333]' --glitchimpact='[33,50,90]' \
        --glitchdecay=0 --glitchmaxlife='[250,1000]' --glitchspan=1 \
        --shockfreq=0 --shockimpact=50 --shockspan=1 --shockmaxlife=10000 \
        --docsize=50 --shelfsize=1 --nseeds=21 --shortlog \
        --listonly 2>&1  > tmp/broker_bigjob_201705191800.log 
    but without all the backslashes; just one extremely long line.
- DON'T FORGET THE AMPERSAND!  or you'll have to use ctrl-Z and bg.


