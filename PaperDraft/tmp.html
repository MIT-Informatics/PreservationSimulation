<h1 id="short-outline-for-short-article">Short outline for short article</h1>
<h4 id="rblandau-20170914">RBLandau 20170914</h4>
<hr />
<h2 id="executive-summary">Executive Summary:</h2>
<ul>
<li><p>(assertion) Digital document collections cannot be safeguarded simply by making a few copies; too many copies are required. The curator needs to audit the integrity of the several copies at intervals.</p></li>
<li><p>(technique) If a copy of a document is corrupted or lost, it can be repaired from other extant copies. A document is permanently lost only when all of its copies have been damaged or lost.</p></li>
<li><p>(assertion) It is important that the several copies of documents be stored independently, so that no single incident -- natural disaster, regional conflict, local terrorist attack, government censorship, local economic downturn, business failure, business realignment, etc. -- is likely to affect multiple copies.</p></li>
<li><p>(scope assertion) This paper does not deal explicitly with threats posed by inimical or incompetent human agencies. Deliberate hacking and poor management practices are beyond our current scope.</p></li>
<li><p>(motivation) The quality of digital storage media is highly variable, particularly over long periods of time. Stored data may deteriorate slowly, in small pieces such as disk sectors, or may fail in large blocks, such as disks, disk arrays, or entire storage services. Redundant storage techniques such as RAID protect against only some such failures.</p></li>
<li><p>(assertion) A curator cannot always assess the reliability of storage of a particular type or location. A strategy to preserve digital documents must be robust over a wide range of reliability and physical conditions.</p></li>
<li><p>(assertion) This paper presents simulation results that suggest that a reasonably small number of copies, audited and repaired regularly, suffice to preserve digital document collections over a very wide range of error conditions.</p></li>
<li><p>(assertion) Document auditing methods must examine every document at some interval. Methods that examine only random subsets of documents sampled with replacement leave some documents unaudited and vulnerable to loss.</p></li>
<li><p>(assertion) Moderately frequent auditing of documents, e.g., annual auditing, suffices to preserve document integrity.</p></li>
<li><p>(assertion) Document size, type, and fragility due to, e.g., encoding, compression, or encryption, need not be significant factors in the choice of storage methodology.</p></li>
<li><p>(assertion) It is possible to divide collections into subsets by, e.g., value, and choose different storage strategies for the subsets. High value documents may be stored more widely and audited more frequently than low value documents.</p></li>
<li><p>(conclusion) To preserve collections of digital documents, store a modest number of copies on independent storage services and audit their contents at regular intervals. This strategy can protect collections over long periods and against a wide variety of storage quality levels, short term quality variations, and large scale failures.</p></li>
</ul>
<hr />
<h2 id="outline">Outline</h2>
<h3 id="the-problem">The Problem</h3>
<ul>
<li>Most digital data is stored on disks. Disks fail, a little at a time or all at once. Disk storage services can also fail, slightly or totally.<br />
</li>
<li>Large collections of digital documents need to be preserved perfectly, or almost perfectly, for long periods of time.</li>
</ul>
<h3 id="the-approach">The Approach</h3>
<ul>
<li>copies of docs</li>
<li>commercial services or your own datacenters</li>
<li>audit docs on a regular schedule</li>
<li>segmented systematic sampling okay, but random sampling with replacement is not</li>
<li>segmentation can even out bandwidth requirements, and is slightly more effective</li>
</ul>
<h3 id="the-simulations">The Simulations</h3>
<ul>
<li>fixed collection size, fixed duration</li>
</ul>
<h3 id="simulation-parameters">Simulation Parameters</h3>
<ul>
<li>doc size can vary but scales linearly, predictably</li>
<li>storage shelf size can vary but scales linearly, predictably</li>
<li>range of independent copies</li>
<li>wide range of error rates, to account for disk technology, manufacturing, and batch variations; reliable, or even plausible, numbers are very difficult to obtain from industry</li>
<li>variety of auditing strategies: frequency, segmentation, sampling without or with replacement</li>
<li>minor glitches for short term physical problems that increase bit error rates on disks, e.g., HVAC: frequency, impact level, duration</li>
<li>shocks for large scale problems that affect entire services: frequency, impact level, duration.</li>
</ul>
<h3 id="variations-to-test-for-robustness">Variations to Test for Robustness</h3>
<ul>
<li>vary number of copies allocated to independent storage services</li>
<li>vary error rates for small disk errors</li>
<li>vary auditing strategy</li>
<li>vary glitches, frequent or rare, minor or major, short or long, affecting the document error rates on single servers</li>
<li>vary shocks, frequent or rare, minor or major, short or long, affecting the survival of one or more servers at a time</li>
<li>vary doc size and error rate together: larger docs should predictably represent bigger targets for random errors</li>
<li>simulations performed with modest sample sizes; occasionally subsampled with much larger sample sizes for validation</li>
<li>simulations performed with repeatable seeds for pseudorandom number generator</li>
</ul>
<h3 id="results">Results</h3>
<ul>
<li>auditing essential to limit copies</li>
<li>auditing should be regular and complete</li>
<li>baseline auditing: five copies, annual, total (all copies each cycle), may be segmented</li>
<li>excessive auditing is overkill</li>
<li>moderate glitches are similar to increased error rate</li>
<li>baseline auditing can protect even against moderate shocks</li>
</ul>
<h3 id="software-available-soon-rsn">Software Available Soon (RSN)</h3>
