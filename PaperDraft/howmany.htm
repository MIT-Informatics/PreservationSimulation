<head>
<style>
th {
background-color: yellow;
font-family: sans-serif;
}
td {
border: 1px solid black;
}
table {
width: 80%;
border-collapse: collapse;
}
</style>
</head>
<p>% How Many Copies Is Enough? % Micah Altman; Richard Landau % 2016-07-11</p>
<h1 id="title" class="unnumbered">How Many Copies Is Enough?</h1>
<p><strong>Assessing Long-term Durability of Collections Through A Flexible, Replicable Simulation</strong></p>
<blockquote>
<p><strong>Abstract</strong></p>
</blockquote>
<blockquote>
<ul>
<li>How to protect a large, valuable, digital document collection</li>
<li>How many copies do you need to keep it safe?</li>
<li>Where to put the copies?</li>
<li>Are you sure they're still there?</li>
<li>Whether to use compression, encryption, ... ?</li>
<li>Not much hard data on which to base policy decisions</li>
<li>We developed a flexible, replicable simulation framework</li>
<li>We aim to provide some guidance, based on some common, but not universal, calibration points.</li>
</ul>
</blockquote>
<h1 id="motivation">Motivation</h1>
<h1 id="fifty-thousand-foot-view">Fifty-Thousand Foot View</h1>
<!-- START:TODO:MICAH-->
<h2 id="problem-definition">Problem Definition</h2>
<ul>
<li>Maintain large digital collections over time</li>
<li>Risks to collections</li>
</ul>
<div class="figure">
<img src="threats.jpg" title="Various Threat Types (in quotes after the link addr)" alt="Various Threats to Library Collections (in the square brackets)" />
<p class="caption">Various Threats to Library Collections (in the square brackets)</p>
</div>
<ol style="list-style-type: decimal">
<li>Document rot on disk.</li>
<li>Environmental failures that accelerate document rot.</li>
<li>Server failures that destroy sets of documents.</li>
<li>Lack of independence of servers.</li>
<li>Attack on collection, institution, subject matter.</li>
<li>Et alia. <!-- END:TODO:MICAH--></li>
</ol>
<!-- DONE! START:TODO:RICK-->
<h2 id="core-assumptions">Core Assumptions</h2>
<ul>
<li>Documents are stored on storage servers, on (currently) rotating disk memories.<br />
</li>
<li>Documents may be lost by becoming unreadable from storage. Such individual document failures are independent of one another.<br />
</li>
<li>A storage server may fail and cause all the data stored on that server to be lost. Such failures are random and occur at some rate. The rate may vary due to exogenous circumstances. Major storage failures are very rare events compared with individual document failures.<br />
</li>
<li>Storage servers are independent of each other. Each server has a characteristic rate of failures of blocks of stored data. Different storage servers may have different failure rates.<br />
</li>
<li>It is possible that the failure rate within a server is not constant over time. However, over suitably short intervals, a changing rate can be approximated by some mean value in the interval.<br />
</li>
<li>Failures of disk data occur in small regions, e.g., blocks of data or small groups of blocks of data. These data failures within a storage service occur randomly and independently among the disk resources of the storage server.<br />
</li>
<li>Documents may occupy more or less storage depending on their size. Since failures of blocks of storage are random and independent, a failure is more likely to be located within a large document than within a small one.</li>
</ul>
<!-- DONE! END:TODO:RICK-->
<h2 id="core-motivating-problem">Core Motivating Problem</h2>
<ul>
<li>Number of choices: formatting, auditing, quality, number of copies ...</li>
<li>Keeping risk of object loss fixed -- what choices minimize $?</li>
<li>&quot;Dual problem&quot; -- Keeping $ fixed, what choices minimize risk?</li>
</ul>
<!-- DONE! START:TODO:RICK-->
<h2 id="basic-model-framework">Basic Model Framework</h2>
<ul>
<li>The model employs one client library with one collection that contains a large number of documents. Results for multiple clients, for clients with multiple collections, or for varying number of documents can be extrapolated from this simple case.<br />
</li>
<li>The client assigns some number of external servers to store a copy of the collection of documents. Each server is supposed to maintain, and be able to retrieve on demand, an authentic copy of any of the documents in the collection.<br />
</li>
<li>Documents may fail on the servers. When a document fails on a server, the failure is silent. The client is not immediately informed of the failure. Indeed, the server might not be able to sense that the failure has occurred until it tries to retrieve the document on a request from the client.<br />
</li>
<li>The model does not consider the costs of storage or bandwidth. These factors vary widely and change rapidly. Any conclusions based on specific numbers would become obsolete very quickly. However, some possibilities for minimizing or smoothing bandwidth consumption are considered.<br />
</li>
<li>From time to time, the client may test the copies of documents on servers to ensure that they can still be read and are still valid copies. The model supports this process of auditing the collection. Audits can be scheduled and performed using a variety of schedules and strategies.<br />
</li>
<li>If in the process of auditing the collection, a document is found to have failed on a server, the client will refresh the failed copy if an intact copy remains on any other server. If no other intact copy remains, the document is considered to be permanently lost.<br />
</li>
<li>At the end of the simulation time period, the model assesses all copies of documents on all servers to determine how many documents have been permanently lost.<br />
</li>
<li>This entire simulation cycle is repeated a number of times using different values to seed the (pseudo-)random number generator that drives the simulation. The numbers from all runs are collected and presented in tabular form and in graphical summaries in the supplemental material.</li>
</ul>
<!-- DONE! END:TODO:RICK-->
<h2 id="illustration">Illustration</h2>
<h1 id="simple-case----independent-failures-just-plain-copies">Simple Case -- Independent Failures &amp; Just Plain Copies</h1>
<p>How many copies do you need if ...</p>
<h2 id="just-make-copies----no-auditing-too-many">Just make copies -- no auditing? TOO MANY</h2>
<!-- START:TODO:RICK -->
<p>TODO: Need guidance about the relationship of so-called MTBF and the half-life that we use. Why did we choose this spectrum, which goes from rusty garbage-can lids to immortal disks.</p>
<!-- outline area

- We chose a region where *something* is going on, some errors but not too many, something that matches experience.  Disks do fail, but not too often.  
- Something related to the Backblaze and Google published numbers.
- Relate drive failures to block failures somehow.  
- Failure rates much lower for RAID, but still silent.  
- Need strategy that works for somewhere on the spectrum, because one never knows where one is on that spectrum, and it changes anyway due to glitches, bad disks, and such.  

Can we calculate backwards from audit results to apparent error rates?  Still wouldn't help with knowing why we are in the spectrum at all, but might be sort of a pleasing confirmation.  

disk failure rates
google
backblaze
how does mtbf relate to observed age failures?
should use mttf for some calcs
ignore bathtub infant failures, observe "useful life" failures and senescence/wearout

disk block error rates
supposed swag for consumer grade sata disks
drive-level bad block replacement rates?  anyone have data?
raid soft error rates?  anyone have data?
raid file migration?  anyone do this?  hard to imagine

document failure rates with raid or error replacement?  
no clue

end of outline area -->
<p>(MUCH HAND-WAVING FOLLOWS)</p>
<h2 id="a-few-words-about-error-rates">A Few Words About Error Rates</h2>
<p>One basic question should be answered before embarking on such simulations: what is the failure rate of stored documents? This is a difficult question due to a lack of real data.</p>
<ul>
<li>There is data on the failure rate of individual disk drives over time. Thanks to Backblaze, Google, and others, there is some published empirical data on failure rates of disk drives of recent technology vintages. These figures refer to replacements of entire disk drives during the useful life and wear-out periods of device use. That is, they exclude infant failures but include mid-life and senescence. Unfortunately, we do not get information on the rates of sector failures, bad block replacements, and so forth.<br />
</li>
<li>There is an estimate of unrecoverable failures on consumer-grade SATA drives that is often mentioned in the industry: Pr{a bit fails during a year} = 1E-14. This looks like a small number until one calculates that a 4TB drive, very common today, contains about 4E13 bits of data, plus essential metadata.</li>
<li>We have not encountered data on the performance of disk drives or blocks in RAID and erasure coding configurations, the effect of pre-emptive data scrubbing, etc.<br />
</li>
<li>Data errors are always assumed to be silent to the client that owns the document. Active searching for and correction of errors is necessary to ensure continuing data integrity. Note that the multiple-copy storage and auditing procedure explored in this paper is analogous to RAID storage with data scrubbing, but done at a document level rather than a block level.</li>
</ul>
<!-- continuing with outline

how to choose a spectrum of error rates?
where something is going on
some error but not too many
between rusty garbage can lid and perfection, between fruit fly and immortality
exponential lifetimes are not consistent with most human experience
somehow relates to our common experience: there are errors but not many
amazon, google, others ever quote rates?  
five nines is absurdly low; nine nines is still way low based on simulations

we have used mean exponential lifetime, which is precisely mtbf or mttf
changed to half-life because it's easier for people to understand
who knows lifetime = 63% of units failed (1-1/e); half-life easier to explain
also, we generally prefer medians as measure of length when distributions are so skewed

-->
<h2 id="the-representation-and-scale-of-error-rates">The Representation and Scale of Error Rates</h2>
<p>The likelihood of an error in a disk bit or sector, or even the failure of an entire disk, is a very small number with many zeroes before the first significant digit. We choose to invert the error rate into a function of lifetime of that bit (or sector). Thus a probability of a bit failing in a year of 1E-14 becomes a mean lifetime of 1E14 years. Expressed that way, the figure seems excessively optimistic. Data on such a disk would be effectively immortal, and that does not correlate with experience. We have to agree with Rosenthal (2010) and others that such estimates are merely marketing projections that are not based on empirical data. Using simulations to investigate such nearly immortal disks would be expensive and fruitless. If there are no errors, then no protective strategy is needed. This does not correlate well with practical experience.</p>
<p>Where, then, to search for information about the effectiveness of replication and auditing of large collections of data? We choose to investigate a region that more nearly matches the experience of computer users, where disks and disk files have lifetimes somewhere between the fruit fly and the universe.</p>
<ul>
<li>There are <em>some</em> errors rather than none.<br />
</li>
<li>Larger data is likely to encounter more errors. But</li>
<li>error rates are not so high as to be crippling to normal usage.</li>
</ul>
<p>The region of error rates that we investigate generates enough errors to evaluate the impact of storing multiple copies and the impact of various auditing strategies. Our conclusions describe storage and auditing strategies that are robust over very wide ranges of error rates (and the corresponding ranges of bit/block/disk lifetimes), spanning approximately four orders of magnitude.</p>
<p>The inverse of error rate is usually expressed in terms of MTBF or MTTF, and, initially, we expressed all parameters as mean exponential lifetime. But MTBF and MTTF are hard for most non-experts to grasp. &quot;By the end of an MTTF period, approximately 63% of the units will have failed&quot; is not easily understood by most non-statisticians. (If we assume Poisson arrivals, the probability of failure in one average lifetime is (1-1/e).) We have chosen for all simulations and tables of results to express lifetime instead as half-life. &quot;By the end of a half-life period, approximately half of the units will have failed&quot; is easier to understand, and should be familiar to most people from examples of radioactive decay.</p>
<p>(END OF HAND-WAVING, AT LEAST ON THIS TOPIC)</p>
<!-- END:TODO:RICK -->
<ul>
<li><p>Long term -- bit rot</p>
<p>o Show results from long term simulation o [TABLE] (how long for 1% loss, based on number of copies) o Interaction -- fragility of big documents o [FIGURE] (how long for 1% loss, based on increasing size) o Cite to Rosenthal previous results on this</p></li>
<li><p>Medium term -- if storage error rates are uncertain</p>
<p>o Storage error rates are difficult to verify o [FIGURE] How long for failure of 1% as error rate increases? o How to interpret claimed storage error rates - What are the limitations of how MTBF is measured? - Given an MTBF, what is the possible bounded range of half-lives?</p></li>
</ul>
<!-- DONE! START:TODO:RICK -->
<h1 id="what-if-you-add-good-auditing-strategies...-five">What if you add good auditing strategies... FIVE</h1>
<!--
- What's "good auditing?"
- Key conditions for this solution
- entirely independent
- (no correlated failures, no intelligent adversaries, no institutional failures) 
- [FIGURE]
- Auditing is systematic 
    1.  (compare to random, usage base)
    2.  Random auditing, with replacement, is less effective.  
    3.  Systematic auditing, periodically and without replacement, is most effective.
        a. Auditing may be performed in segments, e.g., an annual audit can be broken into halves, one half the collection every half year, either systematically selected or randomly selected without replacement; or one quarter of the collection every quarter, and so forth.  
        a. Auditing and egress charges -- piecemeal is ok
        b. Auditing charges would be reduced by cryptographic affordances on cloud-server side ...
- Robustness 
     1,
        Robust to audit frequency
        a. The impact of the rate of auditing is surprisingly less influential than the auditing strategy.  Auditing more frequently than annually has little impact on losses, across a wide spectrum of error rates.
        b. Systematic auditing in a small number of segments, e.g., auditing one quarter of the collection every calendar quarter, is slightly more effective than one large, annual audit, and eases bandwidth requirements. 
     2. Robust to storage quality, storage quality variations over time
     3. NOT robust to  failures associated across servers... 
- [FIGURE]
-->
<p>Auditing the collection, that is, testing the validity of remote copies of documents, can greatly reduce permanent document losses over time. The auditing process actively searches for errors before they cause permanent document losses, and corrects them whenever possible. A number of strategies for auditing are possible, and some are measurably better than others.</p>
<p>In all cases, when a document copy is found to be absent (or corrupted), the auditing process attempts to replace the missing copy with a fresh copy obtained from another server. If there is an intact copy on another server, then the missing document is repaired and the process continues.</p>
<p>Common auditing strategies:</p>
<ul>
<li>Total auditing: test all copies of all documents in the collection. This auditing cycle is usually done systematically, at regular intervals, such as annually, quarterly, monthly, etc.<br />
</li>
<li><p>(Systematic) segmented auditing: divide the collection into several segments, and test one segment at each interval. For example, the collection may be divided into four segments; if each segment in turn is audited at quarterly intervals, then the entire collection will have been audited at the end of a yearly auditing cycle.</p>
<p>Note that segments need not be fixed portions of the collection. Each segment of the collection might be selected at random when its turn comes, so long as the random selection is made <em>without</em> replacement over the audit cycle. This ensures that every document in the collection will be audited exactly once during the complete cycle.</p></li>
<li>Random auditing: at some interval, audit a random subset of documents chosen from the collection. This often is expressed as, for instance, &quot;audit ten percent of the documents every month.&quot; The difference between this random strategy and segmented auditing is that the random selection is chosen <em>with</em> replacement. Thus it is likely that some documents will escape auditing entirely for long periods.<br />
</li>
<li><p>Auditing by popularity: divide the collection into segments that represent varying levels of document usage, e.g., small segments for the documents most frequently accessed, segments for documents of intermediate popularity, and large segments for documents rarely accessed.</p></li>
</ul>
<p>Our simulations include tests of many auditing strategies, including total, segmented, and random. Tests differed in cycle frequency and in the parts of the collection are audited during each segment or cycle.</p>
<ul>
<li>All tests occurred on regular schedules.<br />
</li>
<li>Auditing cycles varied from monthly to biennially.<br />
</li>
<li>Segment counts were either one, two, four, or ten.<br />
</li>
<li>Segments were chosen either systematically (the first quarter of the collection, the second quarter of the collection, etc.) or by uniform random selection.</li>
</ul>
<p>Some features of the results are apparent.</p>
<ul>
<li>Total auditing of the collection is highly effective at reducing document losses.<br />
</li>
<li><p>Auditing in multiple segments is very slightly more effective than auditing the entire collection as one segment; e.g., auditing a quarter of the collection each quarter is slightly more effective than a single annual audit of the whole collection.</p>
<p>We note also that auditing in a number of segments has the additional advantage of spreading the bandwidth requirements for auditing throughout the audit cycle.</p></li>
<li>Random auditing, where segment contents are selected with replacement, is less effective than total auditing or, equivalently, segmented auditing without replacement.<br />
</li>
<li>Across a wide range of document error rates, increasing auditing frequency beyond a certain point shows little improvement.<br />
</li>
<li>The effectiveness of auditing is robust across a wide spectrum of storage quality (i.e., document error rates) and short term variations in storage quality.<br />
</li>
<li><p>However, auditing strategies are not robust to associated failures that compromise multiple servers over short periods.</p></li>
</ul>
<!-- DONE! END:TODO:RICK -->
<h1 id="how-many-more-copies-...-associated-failures">How many more copies ... ? Associated Failures</h1>
<h2 id="type-of-threats">Type of Threats</h2>
<ul>
<li>Server-side Billing Failure<br />
</li>
<li>Server-side Financial Failure<br />
</li>
<li>Unsophisticated adversary outsider attacker<br />
</li>
<li>HVAC Failure/anticipated environmental problem<br />
</li>
<li>Unanticipated Environmental Catastrophe (including local war)<br />
</li>
<li>Local software failure<br />
</li>
<li>Admin failure<br />
</li>
<li>Hardware batch quality<br />
</li>
<li>Formal Government Action<br />
</li>
<li>Powerful External Adversary<br />
</li>
<li>Economic Recession<br />
</li>
<li>Limited Internal Adversary<br />
</li>
<li>Curatorial Failure/Client error<br />
</li>
<li>Common software failure</li>
</ul>
<h2 id="modeling-associated-failures">Modeling Associated Failures</h2>
<p>Sources of failures are modeled as a stochastic processes, in a hierarchical model</p>
<table style="width:71%;">
<colgroup>
<col width="8%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Logical Block</th>
<th>Server (Provider) Glitch</th>
<th>Global Shock</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Represents</td>
<td>failure of logical block within physically raided storage</td>
<td>event affecting reliability of single provider</td>
<td>event affecting reliability of multiple providers</td>
</tr>
<tr class="even">
<td>Distribution</td>
<td>Poisson IID</td>
<td>Poisson IID</td>
<td>Poisson IID</td>
</tr>
<tr class="odd">
<td>Duration</td>
<td>Instantaneous and permanent</td>
<td>Bounded Duration %GLITCH_MAX_LIFE%, Exponential Decay</td>
<td>instantaneous</td>
</tr>
<tr class="even">
<td>Effect</td>
<td>loss of single block of single copy of document</td>
<td>Increases logical block failure rate</td>
<td>(Immediately) inject glitch in k servers</td>
</tr>
<tr class="odd">
<td>Detection</td>
<td>Loss is detected on audit</td>
<td>Server error itself detected on audit iff. block error rate &gt; %CLIENT_SENSITIVITY</td>
<td>Invisible (detected only through effects on block failure and server glitch)</td>
</tr>
<tr class="even">
<td>Notes</td>
<td>Failure rate is not known precisely to client</td>
<td>Induces additional block failures, correlations among block failures.</td>
<td>Induces server glitches, and correlations among server</td>
</tr>
</tbody>
</table>
<p>A number of sources are not modeled, but are assumed to be addressed through storage practices:</p>
<ul>
<li>Bathtub curve, accelerate start and end-of-life failures. The model is conditioned on good systems administration practice is in place, including equipment burn-in and scheduled replacement within the expected service life. Thus the error rate observed during the service life of the equipment should not be subject to these failure.</li>
<li>Raid configuration characteristics, internal raid errors. Standard storage practice may include low-level physical redundancy. Thus the reliability of a logical block will be better than that of an underlying physical storage. When using the model, one should calibrate error rates based on the implied or observed failure rates of logical blocks -- each of which may be represented by redundant physical storage.</li>
<li>Media format obsolence -- we assume good practice -- migration to new media bewfore end of life.</li>
<li>Unmanaged File Format obsolescence.
<ul>
<li>Management can mitigate file format obsolescence where documents are stored in multiple formats (or multiple indepent readers) and tested for format characteristics at audit. Failures occur at the level an entire document, but the threat of loss could be modeled as with a block error rate implying a certain document failure rate for fixed size documents, where the number of servers represents the number of independent formats stored.</li>
<li>Unmanaged format obsolescence cannot be addressed through redundancy, etc. -- will not show on audits</li>
</ul></li>
</ul>
<p>What are are not modeling - Sophisticated adversaries that attack the auditing mechanism - Cascading failure/contagion - Environmental glitch that directly affect background rate of server glitch - Server characteristics are in a steady -state equlibrium -- characteristic of servers remain the same over time.</p>
<p>A wide range of real-world threats may be modeled through varying the parameterization of the model</p>
<h2 id="threat-matrix">Threat Matrix</h2>
<p>A wide range of real-world threats may be modeled through varying the parameterization of the model</p>
<p>[Well, I'm fairly convinced that pandoc markdown can't do complex lists inside tables, so we will have to render this sort of table in raw HTML.]</p>
<table>
<thead>
<tr class="header">
<th>Model Level</th>
<th>Real World Threat Source</th>
<th>Used to predict ...</th>
<th>Use to derive ...</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
| Logical Block |
<ul>
<li>
loss due to media failure
</li>
<li>
loss due to raid/internal replication characteristics and failure
</li>
</ul>
<p>| Document loss rate as a function of {number of replicas, auditing strategy, auditing frequency, block error rate} | Document loss as a function of * document size * format fragility - file compression - managed format obsolescence | | Server | - Server-side Billing Failure - Server-side Financial Failure - Unsophisticated adversary outsider attacker - HVAC Failure/anticipated environmental problem - Unanticipated Environmental Catastrophe (including local war) - Local software failure - Admin failure - Hardware batch quality | Document loss rate rate as a function of server error characteristics, given a fixed choice of {replicas, auditing, block error} Increased redundancy needed to maintain fixed loss rate in presence of server errors, given recommended auditing and repair strategy |</p>
<p>| Global |- Formal Government Action - Powerful External Adversary - Economic Recession - Limited Internal Adversary - Curatorial Failure/Client error - Common software failure | Document loss rate rate as a function of global error characteristics, given a fixed choice of {replicas, auditing, block error} | Increased redundancy needed to maintain fixed loss rate in presence of global errors |</p>
<p>Effects of encryption key escrow policies</p>
<p>Tradeoff between regional diversification and adding servers.</p>
<p>Server Error Parameterizations</p>
<table style="width:89%;">
<colgroup>
<col width="15%" />
<col width="19%" />
<col width="18%" />
<col width="18%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>frequency</th>
<th>Impact</th>
<th>lifetime</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Server Billing</td>
<td>Medium</td>
<td>High ((&gt; sensitivity rate)</td>
<td>Permanent loss of content</td>
<td>Loss of entire collection on server</td>
</tr>
<tr class="even">
<td>Financial</td>
<td>Low</td>
<td>High</td>
<td>Permanent -&gt; simulation period</td>
<td>Bankruptcy - loss of collection</td>
</tr>
<tr class="odd">
<td>Low Resource External Adversary</td>
<td>Low</td>
<td>Medium</td>
<td>Medium</td>
<td>Assume that adversary does not subvert audit</td>
</tr>
<tr class="even">
<td>HVAC</td>
<td>high</td>
<td>small</td>
<td>medium</td>
<td></td>
</tr>
<tr class="odd">
<td>Unanticipated Environmental Catastrophe</td>
<td>low</td>
<td>high</td>
<td>short</td>
<td></td>
</tr>
<tr class="even">
<td>Local Software</td>
<td>medium</td>
<td>medium</td>
<td>long</td>
<td></td>
</tr>
<tr class="odd">
<td>Administrator Error</td>
<td>medium</td>
<td>small</td>
<td>short</td>
<td></td>
</tr>
<tr class="even">
<td>Hardware batch quality</td>
<td>Medium</td>
<td>Medium</td>
<td>Long</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="how-many-more-for">How Many More for</h2>
<ul>
<li>Environmental</li>
<li>Institutional Final Failure</li>
<li>Recession</li>
</ul>
<h1 id="generalizations">Generalizations</h1>
<h2 id="overview">Overview</h2>
<h2 id="large-vs-small-documents">Large vs Small documents</h2>
<h2 id="large-vs-small-collections">Large vs Small collections</h2>
<h2 id="compression">Compression</h2>
<ul>
<li>Documents are fragile
<ul>
<li>Compressed, encrypted: small failure makes document unreadable</li>
<li>Variation: Repairable documents</li>
<li>Small failure damages only one segment</li>
</ul></li>
<li>Model this as a set of smaller docs</li>
</ul>
<h2 id="encryption">Encryption</h2>
<p>Deriving encryption key loss:</p>
<ul>
<li>Assume that your expected collection loss with N_S servers is L% , without encryption</li>
<li>Assume N_K copies of keys with</li>
<li>Assume loss of all keys yields 100% loss of collection</li>
<li><p>Assume availability and integrity of key is verified on audit</p>
<p>For example:</p></li>
</ul>
<p>Suppose there are only 2 copies of keys. Is the expected document rate due to encryption key loss is equivalent to the expected document loss in the following scenario? - there are two servers - logical block failure is 0 - audits are annual - servers are subject to glitches of the <em>financial failure</em> form -- permanent, total loss - What is L*= E(L|KL,N_K) - Can this be derived from failure rate of N_S servers with a rate of KL financial failures ## Format Obsolescence</p>
<h1 id="replication-and-extension">Replication And Extension</h1>
<h2 id="value-of-replicability">Value of replicability</h2>
<h2 id="calibration-with-real-world-data----how-to-calibrate">Calibration with real world data -- how to calibrate</h2>
<h2 id="extension-and-parameterization">Extension and parameterization</h2>
<h1 id="recommendation">Recommendation</h1>
<h2 id="for-collection-owner">For collection owner</h2>
<ul>
<li>Use at least 5 copies</li>
<li>Use systematic annual auditing</li>
<li>Do not trust MTBF and other similar measures</li>
<li>Use compression, with known algorithms</li>
</ul>
<h2 id="for-digital-preservation-commons">For digital preservation commons</h2>
<ul>
<li>Develop standards
<ol style="list-style-type: decimal">
<li>with cloud vendors for cryptographic auditing that does not require data egress</li>
<li>Reporting of failure rates</li>
</ol></li>
<li>Sharing reliability of cloud vendors</li>
<li>Sharing information on correlated failures? ## Recommendations for research</li>
<li>Detailed cost models</li>
<li>Strong adversaries</li>
<li>Erasure codes...</li>
</ul>
<h1 id="references">References</h1>
<h1 id="supplementary-materials----hideous-details">Supplementary Materials -- Hideous Details</h1>
<h2 id="statistical-modeling-detail">Statistical Modeling Detail</h2>
<h2 id="simplifying-assumptions-and-scaling">Simplifying assumptions and scaling</h2>
<ul>
<li>One client</li>
<li>Fixed number of docs</li>
<li>Fixed length of simulation; metric years, quarters, months</li>
<li>Identical servers</li>
<li>Simple document failures: bad sector = dead</li>
<li>Doc size, sector size, shelf size</li>
<li>Scaling of doc size and error rate
<ol style="list-style-type: decimal">
<li>Implications for compression</li>
</ol></li>
<li>Logarithmic scaling of error rates and number of copies for efficiency and easy comparisons</li>
<li>Everything arrives Poisson, rates are exponential, rates stated as half-lives
<ol style="list-style-type: decimal">
<li>Sector errors, server failures</li>
<li>Glitches that impact error rate on a server</li>
<li>Shocks that impact life expectancy of servers</li>
</ol></li>
</ul>
<h2 id="simulation-runs">Simulation Runs</h2>
<ul>
<li>Random number generation, fixed seeds</li>
<li>Small-ish to medium repetitions with seed sequences for repeatability</li>
<li>Statistics extracted: median, midmean, trimean</li>
</ul>
<h2 id="software-architecture-details">Software Architecture Details</h2>
<ul>
<li>Main program for a single run, produces detailed log file</li>
<li>Broker program, and associated shell scripts, to schedule many runs across available compute cores</li>
<li>Extraction program to pull important data from log files</li>
<li>R scripts to summarize data into tables</li>
<li>R scripts to produce graphs</li>
<li>All programs CLI-based, run on Linux (Ubuntu server and Cygwin on Windows)</li>
<li>How-to documentation</li>
</ul>
<h2 id="instructions-for-installation-and-software-execution">Instructions for Installation and Software Execution</h2>
<ul>
<li>How to Install on Amazon Web Services</li>
</ul>
<h2 id="how-to-model-various-scenario">How to Model Various Scenario</h2>
