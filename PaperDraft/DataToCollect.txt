Data to be collected

# -------------------------------------
dimensions that vary: 
(all times in metric hours, months, quarters, years; metric year=10000 hours)
(all lifetimes are half-lives)

copies: 1,2,3,4,5,6,7,8,9,10,14,16,20
sector lifetime: 1,2,3,5,10,20,30,50,100,200,300,500,1000,10000

audit frequency: 0(=never),1000,2500,5000,10000,20000
audit type: TOTAL,SYSTEMATIC  (should change name to SEGMENTED)
audit segments: 1,2,4,10,50

glitch frequency: 0(=never),1000,2500,3333,10000,20000,50000,100000
glitch span: 1
glitch impact: 10,33,50,67,90
glitch lifetime: 0(=forever),250,1000,5000,10000,30000
glitch decay: not currently used
glitch ignore level: not currently used

shock frequency: 0(=never),10000,20000,30000,50000,100000
shock span: 1,2,3,4
shock impact: 10,33,50,67,90,100(=instant death)
shock lifetime: 0(=forever),5000,10000,20000,30000,50000,100000
server default lifetime: 0(=forever),50000,100000,200000,500000

document size: 5,50,500,5000
shelf size: 1,10,100

simulation length: 100000
# -------------------------------------



# -------------------------------------
groupings of data to collect:

Q: how many copies needed if no auditing
BASELINE NOAUDIT
af0 
gf0 sf0 sdl0
cop1-10
lif1-10000

Q: how many copies needed with annual auditing
Q: with annual auditing, five copies suffice over wide range of lifetimes
BASELINE AUDITANNUALLY
af10000 TOTAL s1
gf0 sf0 sdl0
cop1-8
lif1-10000

Q: effectiveness of more frequent total auditing
AUDIT FREQUENCY
af1000,2500,5000,10000 TOTAL s1
gf0 sf0 sdl0
cop2-5
lif10-1000

Q: effectiveness of segmented auditing
AUDIT SEGMENTS
af10000 TOTAL s1,2,4,10,50
gf0 sf0 sdl0
cop2-5
lif10-1000

Q: ineffectiveness of random auditing with replacement
AUDIT RANDOM
af10000 TOTAL s1,2,4,10
af10000 RANDOM s2,4,10
gf0 sf0 sdl0
cop2-5
lif3-1000

Q: docsize and error rate scale linearly against each other
SCALING DOCSIZE
af0
gf0 sf0 sdl0
cop1
docsize5,50,500,5000
lif1-10000

Q: shelfsize irrelevant, only wastes time
SCALING SHELFSIZE
af10000 TOTAL s1
gf0 sf0 sdl0
cop3-5
shelfsize1,10,100
extract hits, misses per shelf, cputime

Q: glitches are just like short-term increased error rate
GLITCHES LARGE
af10000 TOTAL s1
gf2500 gl250,1000 gi33,50,67
gf5000 gl1000,2500 gi33,50,67
gf10000,20000 gi?
cop5
lif10-1000
sf0 sdl0

Q: low-rate shocks are a lot like large glitches
SHOCKS LOW


Q: large shocks and dependencies are dangerous
SHOCKS HIGH
af10000 TOTAL s1
sf10000 sml500,100 si50,90,100
sf20000,30000 sml5000,10000 si50,90,100
sf50000 sml10000,20000 si50,90,100
sf10000,20000 sml10000,20000
cop3,4,5
lif10-1000
sf0 sdl10000



Q: median, trimean, midmean suffice
LOCATION SMALLSAMPLE


LOCATION MEDIUMSAMPLE


LOCATION LARGESAMPLE


Q: collection size irrelevant, care only about percentage losses
SCALING COLLECTIONSIZE
















af0 
gf2500 gl250,1000 gi33,50,67
gf5000 gl1000,2500 gi33,50,67
gf10000,20000 gi?
sf0 sdl0
cop1-8
lif1-10000




af10000 s1,2,4,10
gf0 sf0 sdl0
cop1-5
lif1-1000


[[lots more to add here]]

