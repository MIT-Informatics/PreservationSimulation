Assertions that will require pictures, tables of data, or lengthy discussions to convince the reader.

                RBLandau 20170723


assertion: without auditing, many copies needed to prevent perm losses.  
needs picture to show loss rates, without auditing, at 2, 5, 20, 20 copies.  
- picture: straight Poisson: copies=1 losses y over sector half-lives x.
- table: calibration against pure Poisson for copies=1 with no auditing, showing percentage error.  use large sample size (400? 1000?) to minimize variance.  Can we find a closed form calculation for copies=2?
- table: calibration, pointing out that order of magnitude increases in lifetime generate order of magnitude decreases in losses, nice and linear.

assertion: random auditing less effective than total or segmented total. 
- needs picture of random vs total over same ranges.
- Variant: compare to 5 copies, systematic auditing, how much random auditing to generate loss
- Variant: lose some substantial part of the collection (e.g. 10%) even if auditing up to  X% monthly

assertion: conditioned on 5 copies; increasing audit freq beyond some point gives little improvement across a wide range of error rates.  
- needs picture of biennial, annual, semiannual, quarterly, monthly, maybe even weekly.
- picture: quarterly or faster auditing not noticeably better than annual.  
- picture: auditing with many segments, down to weekly.

assertion: auditing more frequently than annually does not confer much additional protective benefit.  
- needs picture, probably same or subset of above.  

assertion: random auditing that does not cover the collection, i.e., with replacement, is less effective than strategy that touches every document.  
- needs picture comparing random with total with lots of segments.

assertion: auditing robust across range of error rates.  
- needs pictures of annual auditing for copies 3, 4, and 5 across at least two orders of magnitude.  

assertion: five copies with auditing suffices over wide range.
- picture: annual audit with 5 copies reduces losses to negligible, maybe zero, over very large range of sector lifetimes.  compare with 2, 3, 4 copies, i think.

assertion: auditing robust across short term variations in storage quality. (glitches)  
- needs picture, dunno what yet.  

question: auditing robustness in the face of shocks that kill multiple servers.  
- get numbers. 

assertion: document size and error rate scale together linearly.
- picture: document size scales with sector half-life. life 10 size 5 = life 100 size 50 = life 1000 size 500; or did i get that backwards? do all these with copies=1 and no auditing.
- related assertion: use compression to minimize target size. (compare with fragility -- number of docs doc size) great for txt, html, xls.  Note: that jpg, mpeg, pdf, exe -- compression choices are often internal.


assertion: glitches equivalent to slightly shorter sector lifetime.  
- picture or table of very minor increments in losses with smallish glitches.  

assertion: minor shocks equivalent to very large glitches.

table: disk drive error rates from backblaze, Google, et al., just samples of numbers all over the place. 
- Connect to rusty-garbage-can-lid spectrum.  Eg if sector error rate > disk drive error * K, how small does K have to be before you can turn off auditing and not worried?  

table: Poisson theoretical vs simulated numbers of errors for range of sector lifetime, sector size, document size, simulation length.

how to model
- corollary -- how to mitigate -- if the baseline is 5 copies + quarterly auditing, how many more copies would you need to keep to ensure zero loss in the face of these threats

model: billing failure or government takedown: shock with 100% impact (instantly fatal); short frequency; span 1, 2, 3.

model: financial recession: shocks with various frequencies; various impacts < 100%; various spans.

model: low resource external adversary: dunno.

model: HVAC failures, flaky power: glitches with frequency high; impact low to medium; span 1; lifetimes medium.  

model: environmental catastrophe: shocks; frequency low; impact high or fatal; span low; duration med.  

model: local software failure: shocks; freq med; impact med; span high; duration long.

model: administrator error: shocks; freq med; impact med; span low; duration med.

model: hardware batch poor quality: glitches; freq med; impact med; span 1; duration long.

model: institutional failure or redirection: shocks; freq low; impact fatal; span 1; duration n/a.

model: institutional merger: shocks; freq med; impact fatal; span 1; duration n/a.

model: document compression or encryption: smaller doc size; otherwise same.  Since doc size and sector lifetime are linearly related, this can be calculated rather than simulated.  

model: collection size: loss *rate* is independent of number of docs, so linear scaling is appropriate.  Could adjust the ndocs param and spend more computer time to verify.  

model: more vs less important docs: calc for two separate collections, one small with no losses permitted, and one large with some acceptable loss rate.  Adjust number of copies and audit rate accordingly for each collection.  

assertion: doc size and failure rate scale linearly against each other.  need simple Poisson formula and tables for copies=1.  also need table or graph for multiple copies with auditing, to show similar relationship.  

model: docs of different sizes: calc with different sizes and combine results.  

assertion: losses for different doc sizes are additive. 

picture or table: increased size of doc vs number of copies for fixed loss rate.

assertion: five copies with annual auditing is enough across huge range of failure rate.  table and picture.

assertion: don't trust MTBF.

assertion: storage structure size irrelevant, affects only the miss rate for random errors.  
- argument: bigger target merely has more empty space for errors to hit.  small storage structures or big docs merely make more structures (shelves) available for errors, but the error rate is constant.

assertion: sector size is irrelevant, scales linearly as part of Poisson calculation.  
- argument: show the arithmetic.  have an xls that does this.  

assertion: doc size, error rate, and storage extent size all scale linearly.  

assertion: fixed set of random seeds sufficient. 
- argument: well, it's certainly good for reproducibility and repeatability.  

assertion: median, trimean, midmean suffice.  

assertion: collection size irrelevant, only concerned with fraction of docs that fail, e.g., 1%, 0.1%, 0.01%, etc.

model: no audit, annual audit, biennial audit, quarterly audit, monthly audit; annual audit segmented quarterly or monthly.






