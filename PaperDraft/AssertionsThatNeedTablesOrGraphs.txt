Assertions that will require pictures, tables of data, or lengthy discussions to convince the reader.

                RBLandau 20170723


assertion: random auditing less effective than total or segmented total.

assertion: increasing audit freq beyond some point gives little improvement across a wide range of error rates.

assertion: auditing more frequently than annually does not confer much additional protective benefit.

assertion: random auditing that does not cover the collection, i.e., with replacement, is less effective than strategy that touches every document.

assertion: auditing robust across range of error rates.

assertion: auditing robust across short term variations in storage quality. (glitches)

question: auding robustness in the face of shocks that kill multiple servers.

assertion: without auditing, many copies needed to prevent perm losses.

picture: straight Poisson: copies=1 losses y over sector half-lives x.

picture: document size scales with sector half-life. life 10 size 5 = life 100 size 50 = life 1000 size 500; or did i get that backwards? do all these with copies=1 and no auditing.

picture: annual audit with 5 copies reduces losses to negligible, maybe zero, over very large range of sector lifetimes.  compare with 2, 3, 4 copies, i think.

picture: quarterly or faster auditing not noticeably better than annual.

picture: glitches equivalent to slightly shorter sector lifetime.
even minor shocks equivalent to very large glitches.

table: disk drive error rates from backblaze, google, et al., just samples of numbers all over the place.

table: Poisson theoretical vs simulated numbers of errors for range of sector lifetime, sector size, document size, simulation length.

model: billing failure or government takedown: shock with 100% impact (instantly fatal); short frequency; span 1, 2, 3.

model: financial recession: shocks with various frequencies; various impacts < 100%; various spans.

model: low resource external adversary: dunno.

model: HVAC failures, flaky power: glitches with frequency high; impact low to medium; span 1; lifetimes medium.  

model: environmental catastrophe: shocks; frequency low; impact high or fatal; span low; duration med.  

model: local software failure: shocks; freq med; impact med; span high; duration long.

model: administrator error: shocks; freq med; impact med; span low; duration med.

model: hardware batch poor quality: glitches; freq med; impact med; span 1; duration long.

model: institutional failure or redirection: shocks; freq low; impact fatal; span 1; duration n/a.

model: institutional merger: shocks; freq med; impact fatal; span 1; duration n/a.

model: document compression or encryption: smaller doc size; otherwise same.  Since doc size and sector lifetime are linearly related, this can be calculated rather than simulated.  

model: collection size: loss *rate* is independent of number of docs, so linear scaling is appropriate.  Could adjust the ndocs param and spend more computer time to verify.  

model: more vs less important docs: calc for two separate collections, one small with no losses permitted, and one large with some acceptable loss rate.  Adjust number of copies and audit rate accordingly for each collection.  

assertion: doc size and failure rate scale linearly against each other.  need simple Poisson formula and tables for copies=1.  also need table or graph for multiple copies with auditing, to show similar relationship.  

model: docs of different sizes: calc with different sizes and combine results.  

assertion: losses for different doc sizes are additive. 

picture or table: increased size of doc vs number of copies for fixed loss rate.

table: calibration against pure Poisson for copies=1 with no auditing.  use large sample size (400? 1000?) to minimize variance.  Can we find a closed form calculation for copies=2?

assertion: five copies with annual auditing is enough across huge range of failure rate.  table and picture.

assertion: don't trust MTBF.

assertion: use compression to minimize target size.  great for txt, html, xls.  unfortunately doesn't help much with jpg, mpeg, pdf, exe.  

assertion: storage structure size irrelevant, affects only the miss rate for random errors.  

assertion: sector size is irrelevant, scales linearly as part of Poisson calculation.  

assertion: doc size, error rate, and storage extent size all scale linearly.  

assertion: fixed set of random seeds.  

assertion: median, trimean, midmean suffice.  

assertion: collection size irrelevant, only concerned with fraction of docs that fail, e.g., 1%, 0.1%, 0.01%, etc.

model: no audit, annual audit, biennial audit, quarterly audit, monthly audit; annual audit segmented quarterly or monthly.






